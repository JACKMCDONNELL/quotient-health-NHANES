{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19780287-c46f-468d-ab02-bf571b7efb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading DEMO (DEMO_I.xpt)...\n",
      "  Loaded DEMO: (9971, 47)\n",
      "Downloading HDL (HDL_I.xpt)...\n",
      "  Loaded HDL: (8021, 3)\n",
      "Downloading TCHOL (TCHOL_I.xpt)...\n",
      "  Loaded TCHOL: (8021, 3)\n",
      "Downloading TRIGLY (TRIGLY_I.xpt)...\n",
      "  Loaded TRIGLY: (3191, 6)\n",
      "Downloading GLU (GLU_I.xpt)...\n",
      "  Loaded GLU: (3191, 4)\n",
      "Downloading INS (INS_I.xpt)...\n",
      "  Loaded INS: (3191, 7)\n",
      "Downloading DPQ (DPQ_I.xpt)...\n",
      "  Loaded DPQ: (5735, 11)\n",
      "Downloading SLQ (SLQ_I.xpt)...\n",
      "  Loaded SLQ: (6327, 8)\n",
      "Downloading DR1TOT (DR1TOT_I.xpt)...\n",
      "  Loaded DR1TOT: (9544, 168)\n",
      "Downloading DR1IFF (DR1IFF_I.xpt)...\n",
      "  Loaded DR1IFF: (121481, 84)\n",
      "Downloading PAQ (PAQ_I.xpt)...\n",
      "  Loaded PAQ: (9255, 94)\n",
      "Downloading BPX (BPX_I.xpt)...\n",
      "  Loaded BPX: (9544, 21)\n",
      "Downloading BIOPRO (BIOPRO_I.xpt)...\n",
      "  Loaded BIOPRO: (6744, 38)\n",
      "Downloading ALB_CR (ALB_CR_I.xpt)...\n",
      "  Loaded ALB_CR: (8608, 8)\n",
      "\n",
      "After merging person-level NHANES files: (9971, 406)\n",
      "FPED shape: (9544, 51)\n",
      "FPED first few columns: ['SEQN', 'RIAGENDR', 'RIDAGEYR', 'RIDRETH3', 'SDMVPSU', 'SDMVSTRA', 'INDFMIN2', 'INDFMPIR', 'WTDRD1', 'WTDR2D', 'DR1DRSTZ', 'DRABF', 'DRDINT', 'DR1TNUMF', 'DR1T_F_CITMLB', 'DR1T_F_OTHER', 'DR1T_F_JUICE', 'DR1T_F_TOTAL', 'DR1T_V_DRKGR', 'DR1T_V_REDOR_TOMATO']\n",
      "After merging FPED: (9971, 456)\n",
      "Duplicate variable bases found: {'INDFMPIR', 'SDMVSTRA', 'WTSAF2YR', 'DRABF', 'RIDAGEYR', 'DRDINT', 'RIDRETH3', 'DR1DRSTZ', 'DR1TNUMF', 'WTDRD1', 'RIAGENDR', 'SDMVPSU', 'WTDR2D', 'INDFMIN2'}\n",
      "✓ All *_x / *_y duplicates cleaned.\n",
      "Remaining *_x / *_y columns: []\n",
      "Found WTSAF2YR variants: ['WTSAF2YR', 'WTSAF2YR']\n",
      "✓ Cleaned. Remaining WTSAF2YR columns: []\n",
      "\n",
      "Feature table shape: (9971, 455)\n",
      "\n",
      "✓ rPDQS columns present? True True\n",
      "      SEQN  rpdqs_total  rpdqs_normalized\n",
      "0  83732.0         21.0         37.500000\n",
      "1  83733.0          9.0         16.071429\n",
      "2  83734.0         25.0         44.642857\n",
      "3  83735.0         28.0         50.000000\n",
      "4  83736.0         27.0         48.214286\n",
      "\n",
      "Longevity variable matrix shape: (9971, 21)\n",
      "      SEQN  ogtt_norm  apob_norm  vo2max_norm  crp_norm  bmi_norm  \\\n",
      "0  83732.0        NaN        NaN          NaN       NaN       NaN   \n",
      "1  83733.0        NaN    -8.6125          NaN       NaN       NaN   \n",
      "2  83734.0        NaN    -1.6000          NaN       NaN       NaN   \n",
      "3  83735.0        NaN        NaN          NaN       NaN       NaN   \n",
      "4  83736.0        NaN    29.0750          NaN       NaN       NaN   \n",
      "\n",
      "   pack_years_norm  moca_norm  mvpa_norm  cac_norm  ...  phq9_norm  alt_norm  \\\n",
      "0              NaN        NaN        NaN       NaN  ...  96.296296       NaN   \n",
      "1              NaN        NaN        NaN       NaN  ...  92.592593       NaN   \n",
      "2              NaN        NaN        NaN       NaN  ...  96.296296       NaN   \n",
      "3              NaN        NaN        NaN       NaN  ...  51.851852       NaN   \n",
      "4              NaN        NaN        NaN       NaN  ...  70.370370       NaN   \n",
      "\n",
      "    egfr_norm  bmd_norm  truage_norm  shdl_norm  rem_norm  grip_norm  \\\n",
      "0   73.132726       NaN          NaN        NaN       NaN        NaN   \n",
      "1   49.757429       NaN          NaN        NaN       NaN        NaN   \n",
      "2   14.483540       NaN          NaN        NaN       NaN        NaN   \n",
      "3   17.231586       NaN          NaN        NaN       NaN        NaN   \n",
      "4  103.488549       NaN          NaN        NaN       NaN        NaN   \n",
      "\n",
      "   swls_norm  diet_norm  \n",
      "0        NaN  37.500000  \n",
      "1        NaN  16.071429  \n",
      "2        NaN  44.642857  \n",
      "3        NaN  50.000000  \n",
      "4        NaN  48.214286  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import pyreadstat\n",
    "\n",
    "# ============================================================\n",
    "# 0. Helper: Download any NHANES .xpt file\n",
    "# ============================================================\n",
    "\n",
    "def load_nhanes_xpt(file, year=\"2015\"):\n",
    "    \"\"\"\n",
    "    file: e.g. 'DEMO_I.xpt'\n",
    "    year: '2015'  (for 2015–2016 cycle)\n",
    "    \"\"\"\n",
    "    url = f\"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/{year}/DataFiles/{file}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "    r = requests.get(url, headers=headers)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # If server returned HTML (error), fail clearly\n",
    "    if b\"<html\" in r.content[:200].lower():\n",
    "        preview = r.content[:500].decode(errors=\"ignore\")\n",
    "        raise ValueError(f\"HTML returned instead of XPT:\\n{url}\\n\\nPreview:\\n{preview}\")\n",
    "\n",
    "    # write to temp file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".xpt\", delete=False) as tmp:\n",
    "        tmp.write(r.content)\n",
    "        tmp_path = tmp.name\n",
    "\n",
    "    try:\n",
    "        df, meta = pyreadstat.read_xport(tmp_path)\n",
    "    finally:\n",
    "        os.remove(tmp_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Download + merge NHANES 2015–2016 person-level files\n",
    "# ============================================================\n",
    "\n",
    "nhanes_files = {\n",
    "    \"DEMO\":      \"DEMO_I.xpt\",\n",
    "    \"HDL\":       \"HDL_I.xpt\",\n",
    "    \"TCHOL\":     \"TCHOL_I.xpt\",\n",
    "    \"TRIGLY\":    \"TRIGLY_I.xpt\",\n",
    "    \"GLU\":       \"GLU_I.xpt\",\n",
    "    \"INS\":       \"INS_I.xpt\",\n",
    "    \"DPQ\":       \"DPQ_I.xpt\",\n",
    "    \"SLQ\":       \"SLQ_I.xpt\",\n",
    "    \"DR1TOT\":    \"DR1TOT_I.xpt\",   # day 1 total intake\n",
    "    \"DR1IFF\":    \"DR1IFF_I.xpt\",   # food items (we won't use for FPED-based rPDQS)\n",
    "    \"PAQ\":       \"PAQ_I.xpt\",\n",
    "    \"BPX\":       \"BPX_I.xpt\",\n",
    "    \"BIOPRO\":    \"BIOPRO_I.xpt\",\n",
    "    \"ALB_CR\":    \"ALB_CR_I.xpt\"\n",
    "}\n",
    "\n",
    "all_dfs = {}\n",
    "for name, fname in nhanes_files.items():\n",
    "    print(f\"Downloading {name} ({fname})...\")\n",
    "    df_tmp = load_nhanes_xpt(fname, \"2015\")\n",
    "    all_dfs[name] = df_tmp\n",
    "    print(f\"  Loaded {name}: {df_tmp.shape}\")\n",
    "\n",
    "# Start from DEMO as person-level base\n",
    "df = all_dfs[\"DEMO\"].copy()\n",
    "\n",
    "# Merge all other files EXCEPT DR1IFF (we don't want item-level explosion)\n",
    "for name, d in all_dfs.items():\n",
    "    if name in [\"DEMO\", \"DR1IFF\"]:\n",
    "        continue\n",
    "    df = df.merge(d, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "print(\"\\nAfter merging person-level NHANES files:\", df.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load FPED DR1TOT day-1 file and merge\n",
    "#    You already have: fped_dr1tot_1516.sas7bdat in working dir\n",
    "# ============================================================\n",
    "\n",
    "fped, meta_fped = pyreadstat.read_sas7bdat(\"fped_dr1tot_1516.sas7bdat\")\n",
    "fped = fped.copy()\n",
    "\n",
    "print(\"FPED shape:\", fped.shape)\n",
    "print(\"FPED first few columns:\", list(fped.columns[:20]))\n",
    "\n",
    "# Merge FPED (per-person food group equivalents for day 1)\n",
    "df = df.merge(fped, on=\"SEQN\", how=\"left\")\n",
    "print(\"After merging FPED:\", df.shape)\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN ALL *_x / *_y DUPLICATES ACROSS THE ENTIRE DATAFRAME\n",
    "# ============================================================\n",
    "df = df.copy()\n",
    "\n",
    "all_cols = df.columns\n",
    "base_vars = set()\n",
    "\n",
    "# Identify every base variable that has *_x or *_y versions.\n",
    "for c in all_cols:\n",
    "    if c.endswith(\"_x\") or c.endswith(\"_y\"):\n",
    "        base_vars.add(c[:-2])\n",
    "\n",
    "print(\"Duplicate variable bases found:\", base_vars)\n",
    "\n",
    "for base in base_vars:\n",
    "    col_x = base + \"_x\"\n",
    "    col_y = base + \"_y\"\n",
    "\n",
    "    # Case 1 — both exist → keep _x, drop _y, rename _x → base\n",
    "    if col_x in df.columns and col_y in df.columns:\n",
    "        df.drop(columns=[col_y], inplace=True)\n",
    "        df.rename(columns={col_x: base}, inplace=True)\n",
    "\n",
    "    # Case 2 — only _x exists → rename to base if base not already in df\n",
    "    elif col_x in df.columns and base not in df.columns:\n",
    "        df.rename(columns={col_x: base}, inplace=True)\n",
    "\n",
    "    # Case 3 — only _y exists → rename to base if base not already in df\n",
    "    elif col_y in df.columns and base not in df.columns:\n",
    "        df.rename(columns={col_y: base}, inplace=True)\n",
    "\n",
    "print(\"✓ All *_x / *_y duplicates cleaned.\")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "print(\"Remaining *_x / *_y columns:\",\n",
    "      [c for c in df.columns if c.endswith('_x') or c.endswith('_y')])\n",
    "\n",
    "# Now safe to make master copy\n",
    "df_master = df.copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FIX DUPLICATE NHANES DEMOGRAPHIC COLUMNS\n",
    "# Keep the \"_x\" versions and drop the \"_y\" duplicates\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "demo_vars = [\n",
    "    \"RIAGENDR\", \"RIDAGEYR\", \"RIDRETH3\",\n",
    "    \"SDMVPSU\", \"SDMVSTRA\",\n",
    "    \"INDFMIN2\", \"INDFMPIR\",\n",
    "    \"WTDRD1\", \"WTDR2D\", \"DR1DRSTZ\", \"DRABF\", \"DRDINT\", \"DR1TNUMF\"\n",
    "]\n",
    "\n",
    "# Step 1: rename all _x versions back to their base name\n",
    "rename_map = {}\n",
    "for var in demo_vars:\n",
    "    old = var + \"_x\"\n",
    "    if old in df_master.columns:\n",
    "        rename_map[old] = var\n",
    "\n",
    "df_master = df_master.rename(columns=rename_map)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# FIX DUPLICATE WEIGHT VARIABLES *INSIDE df_master*\n",
    "# ----------------------------------------------------\n",
    "\n",
    "wtsaf_cols = [c for c in df_master.columns if c.startswith(\"WTSAF2YR\")]\n",
    "\n",
    "print(\"Found WTSAF2YR variants:\", wtsaf_cols)\n",
    "\n",
    "if len(wtsaf_cols) > 1:\n",
    "    # Combine into a single variable using first non-null value\n",
    "    df_master[\"WTSAF2YR\"] = df_master[wtsaf_cols].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "    # Drop all the original variants\n",
    "    df_master.drop(columns=wtsaf_cols, inplace=True)\n",
    "\n",
    "    print(\"✓ Cleaned. Remaining WTSAF2YR columns:\",\n",
    "          [c for c in df_master.columns if \"WTSAF2YR\" in c])\n",
    "else:\n",
    "    print(\"No duplicates found — nothing to clean.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Feature Engineering (labs, BP, eGFR, sleep, PA, PHQ)\n",
    "# ============================================================\n",
    "\n",
    "df = df_master.copy()  # work on a fresh copy\n",
    "\n",
    "# -----------------------------\n",
    "# 3.1 Basic demographics\n",
    "# -----------------------------\n",
    "df[\"sex\"] = df[\"RIAGENDR\"]               # 1=Male, 2=Female\n",
    "df[\"race_ethnicity\"] = df[\"RIDRETH3\"]    # consistent with Longevity v1.1\n",
    "df[\"age\"] = df[\"RIDAGEYR\"]               # age in years\n",
    "df[\"poverty_income_ratio\"] = df[\"INDFMIN2\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 3.2 Rename biochemical variables\n",
    "# -----------------------------\n",
    "rename_map = {\n",
    "    \"LBXGLU\": \"fasting_glucose\",\n",
    "    \"LBXIN\": \"fasting_insulin\",\n",
    "    \"LBDHDD\": \"hdl_cholesterol\",   # HDL_I\n",
    "    \"LBDHDL\": \"hdl_cholesterol\",   # alt name (not needed here but safe)\n",
    "    \"LBXTC\": \"total_cholesterol\",\n",
    "    \"LBXTR\": \"triglycerides\",\n",
    "    \"LBXSCR\": \"serum_creatinine\"\n",
    "}\n",
    "\n",
    "df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns},\n",
    "          inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3.3 LDL (Friedewald) + ApoB est\n",
    "# -----------------------------\n",
    "def friedewald_ldl(row):\n",
    "    tg = row[\"triglycerides\"]\n",
    "    if pd.isna(tg) or tg >= 400:\n",
    "        return np.nan\n",
    "    return row[\"total_cholesterol\"] - row[\"hdl_cholesterol\"] - tg / 5\n",
    "\n",
    "if {\"total_cholesterol\", \"hdl_cholesterol\", \"triglycerides\"}.issubset(df.columns):\n",
    "    df[\"ldl_cholesterol\"] = df.apply(friedewald_ldl, axis=1)\n",
    "    df[\"apob_est\"] = 0.65 * df[\"ldl_cholesterol\"] + 0.1 * df[\"triglycerides\"]\n",
    "else:\n",
    "    df[\"ldl_cholesterol\"] = np.nan\n",
    "    df[\"apob_est\"] = np.nan\n",
    "\n",
    "# -----------------------------\n",
    "# 3.4 HOMA-IR\n",
    "# -----------------------------\n",
    "if {\"fasting_glucose\", \"fasting_insulin\"}.issubset(df.columns):\n",
    "    df[\"homa_ir\"] = (df[\"fasting_insulin\"] * df[\"fasting_glucose\"]) / 405\n",
    "else:\n",
    "    df[\"homa_ir\"] = np.nan\n",
    "\n",
    "# -----------------------------\n",
    "# 3.5 Blood pressure aggregates\n",
    "# -----------------------------\n",
    "sbp_cols = [c for c in df.columns if c.startswith(\"BPXSY\")]\n",
    "dbp_cols = [c for c in df.columns if c.startswith(\"BPXDI\")]\n",
    "\n",
    "if sbp_cols:\n",
    "    df[\"sbp\"] = df[sbp_cols].mean(axis=1)\n",
    "if dbp_cols:\n",
    "    df[\"dbp\"] = df[dbp_cols].mean(axis=1)\n",
    "\n",
    "if {\"sbp\", \"dbp\"}.issubset(df.columns):\n",
    "    df[\"pulse_pressure\"] = df[\"sbp\"] - df[\"dbp\"]\n",
    "else:\n",
    "    df[\"pulse_pressure\"] = np.nan\n",
    "\n",
    "# -----------------------------\n",
    "# 3.6 eGFR — CKD-EPI 2021 (race-free)\n",
    "# -----------------------------\n",
    "if {\"serum_creatinine\", \"sex\", \"age\"}.issubset(df.columns):\n",
    "    scr = df[\"serum_creatinine\"]\n",
    "    sex = df[\"sex\"]\n",
    "    age = df[\"age\"]\n",
    "\n",
    "    k = np.where(sex == 2, 0.7, 0.9)\n",
    "    alpha = np.where(sex == 2, -0.241, -0.302)\n",
    "\n",
    "    min_part = np.minimum(scr / k, 1) ** alpha\n",
    "    max_part = np.maximum(scr / k, 1) ** -1.2\n",
    "\n",
    "    df[\"egfr\"] = 142 * min_part * max_part * (0.9938 ** age)\n",
    "else:\n",
    "    df[\"egfr\"] = np.nan\n",
    "\n",
    "# -----------------------------\n",
    "# 3.7 Sleep metrics (SLQ030)\n",
    "# -----------------------------\n",
    "if \"SLQ030\" in df.columns:\n",
    "    df[\"sleep_hours\"] = df[\"SLQ030\"]\n",
    "\n",
    "    def sleep_score(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        if 7 <= x <= 9:\n",
    "            return 100\n",
    "        if 6 <= x < 7 or 9 < x <= 10:\n",
    "            return 80\n",
    "        return 50\n",
    "\n",
    "    df[\"sleep_score\"] = df[\"sleep_hours\"].apply(sleep_score)\n",
    "else:\n",
    "    df[\"sleep_hours\"] = np.nan\n",
    "    df[\"sleep_score\"] = np.nan\n",
    "\n",
    "# -----------------------------\n",
    "# 3.8 Physical Activity (leisure MVPA)\n",
    "# -----------------------------\n",
    "if {\"PAD615\", \"PAD630\"}.issubset(df.columns):\n",
    "    vig_min = df[\"PAD615\"] * df[\"PAD630\"]\n",
    "else:\n",
    "    vig_min = np.nan\n",
    "\n",
    "if {\"PAD645\", \"PAD660\"}.issubset(df.columns):\n",
    "    mod_min = df[\"PAD645\"] * df[\"PAD660\"]\n",
    "else:\n",
    "    mod_min = np.nan\n",
    "\n",
    "if not (isinstance(vig_min, float) and isinstance(mod_min, float)):\n",
    "    df[\"mvpa_min_week\"] = vig_min * 2 + mod_min\n",
    "else:\n",
    "    df[\"mvpa_min_week\"] = np.nan\n",
    "\n",
    "# -----------------------------\n",
    "# 3.9 PHQ-9 total\n",
    "# -----------------------------\n",
    "phq_cols = [c for c in df.columns if c.startswith(\"DPQ0\")]\n",
    "if phq_cols:\n",
    "    df[\"phq9_total\"] = df[phq_cols].sum(axis=1)\n",
    "else:\n",
    "    df[\"phq9_total\"] = np.nan\n",
    "\n",
    "# -----------------------------\n",
    "# 3.10 Clean common NHANES codes\n",
    "# -----------------------------\n",
    "df.replace({7777: np.nan, 9999: np.nan, 99999: np.nan, 999999: np.nan},\n",
    "           inplace=True)\n",
    "\n",
    "# At this point df is your main feature-engineered table\n",
    "df_features = df.copy()\n",
    "print(\"\\nFeature table shape:\", df_features.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. rPDQS-style diet score from FPED day 1\n",
    "#    using DR1T_* variables in FPED_DR1TOT_1516\n",
    "# ============================================================\n",
    "\n",
    "df = df_features.copy()\n",
    "\n",
    "# If DR1DRSTZ (recall status) exists, restrict to reliable day 1\n",
    "if \"DR1DRSTZ\" in df.columns:\n",
    "    bad_mask = df[\"DR1DRSTZ\"] != 1\n",
    "else:\n",
    "    bad_mask = pd.Series(False, index=df.index)\n",
    "\n",
    "# List of FPED variables we plan to use\n",
    "fped_vars_needed = [\n",
    "    \"DR1T_F_CITMLB\", \"DR1T_F_OTHER\",\n",
    "    \"DR1T_V_DRKGR\", \"DR1T_V_OTHER\",\n",
    "    \"DR1T_V_REDOR_TOTAL\", \"DR1T_V_STARCHY_TOTAL\", \"DR1T_V_LEGUMES\",\n",
    "    \"DR1T_G_WHOLE\", \"DR1T_G_REFINED\",\n",
    "    \"DR1T_PF_MEAT\", \"DR1T_PF_CUREDMEAT\",\n",
    "    \"DR1T_PF_SEAFD_HI\", \"DR1T_PF_SEAFD_LOW\",\n",
    "    \"DR1T_PF_NUTSDS\",\n",
    "    \"DR1T_D_MILK\", \"DR1T_D_YOGURT\",\n",
    "    \"DR1T_A_DRINKS\", \"DR1T_SOLID_FATS\"\n",
    "]\n",
    "\n",
    "# Guarantee these columns exist (if missing, create with NaN)\n",
    "for col in fped_vars_needed:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Set FPED vars to NaN when recall is unreliable\n",
    "if \"DR1DRSTZ\" in df.columns:\n",
    "    df.loc[bad_mask, fped_vars_needed] = np.nan\n",
    "\n",
    "# --- Construct approximate rPDQS food groups (servings/day) ---\n",
    "df[\"dark_green_veg\"]        = df[\"DR1T_V_DRKGR\"]\n",
    "df[\"other_veg\"]             = (\n",
    "    df[\"DR1T_V_OTHER\"]\n",
    "    + df[\"DR1T_V_REDOR_TOTAL\"]\n",
    "    + df[\"DR1T_V_STARCHY_TOTAL\"]\n",
    "    - df[\"DR1T_V_LEGUMES\"].fillna(0)\n",
    ")\n",
    "df[\"citrus_melons_berries\"] = df[\"DR1T_F_CITMLB\"]\n",
    "df[\"other_fruit\"]           = df[\"DR1T_F_OTHER\"]\n",
    "df[\"legumes\"]               = df[\"DR1T_V_LEGUMES\"]                   # could add PF_LEGUMES if present\n",
    "df[\"whole_grains\"]          = df[\"DR1T_G_WHOLE\"]\n",
    "df[\"nuts_seeds\"]            = df[\"DR1T_PF_NUTSDS\"]\n",
    "df[\"low_fat_dairy\"]         = df[\"DR1T_D_MILK\"] + df[\"DR1T_D_YOGURT\"]\n",
    "df[\"fish\"]                  = df[\"DR1T_PF_SEAFD_HI\"] + df[\"DR1T_PF_SEAFD_LOW\"]\n",
    "\n",
    "df[\"red_meat\"]              = df[\"DR1T_PF_MEAT\"]\n",
    "df[\"processed_meat\"]        = df[\"DR1T_PF_CUREDMEAT\"]\n",
    "df[\"refined_grains\"]        = df[\"DR1T_G_REFINED\"]\n",
    "df[\"ssb\"]                   = df[\"DR1T_A_DRINKS\"]\n",
    "df[\"fried_foods\"]           = df[\"DR1T_SOLID_FATS\"]   # crude proxy\n",
    "\n",
    "healthy_groups = [\n",
    "    \"dark_green_veg\",\"other_veg\",\"citrus_melons_berries\",\"other_fruit\",\n",
    "    \"legumes\",\"whole_grains\",\"nuts_seeds\",\"low_fat_dairy\",\"fish\"\n",
    "]\n",
    "unhealthy_groups = [\n",
    "    \"red_meat\",\"processed_meat\",\"refined_grains\",\"ssb\",\"fried_foods\"\n",
    "]\n",
    "\n",
    "servings = df[[\"SEQN\"] + healthy_groups + unhealthy_groups].set_index(\"SEQN\")\n",
    "\n",
    "def pos_score(s):\n",
    "    \"\"\"Higher intake = better (for healthy groups).\"\"\"\n",
    "    s = s.fillna(0)\n",
    "    if s.nunique() <= 1 or s.notna().sum() < 5:\n",
    "        # If basically no variation, just give everyone 0\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    ranked = s.rank(method=\"first\")\n",
    "    return pd.qcut(ranked, 5, labels=[0,1,2,3,4], duplicates=\"drop\").astype(float)\n",
    "\n",
    "def neg_score(s):\n",
    "    \"\"\"Lower intake = better (for unhealthy groups).\"\"\"\n",
    "    s = s.fillna(0)\n",
    "    if s.nunique() <= 1 or s.notna().sum() < 5:\n",
    "        # If no variation, everyone gets middle-ish score (2) or 0; here choose 0\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    ranked = s.rank(method=\"first\")\n",
    "    return pd.qcut(ranked, 5, labels=[4,3,2,1,0], duplicates=\"drop\").astype(float)\n",
    "\n",
    "# Apply scoring\n",
    "for g in healthy_groups:\n",
    "    servings[g + \"_score\"] = pos_score(servings[g])\n",
    "\n",
    "for g in unhealthy_groups:\n",
    "    servings[g + \"_score\"] = neg_score(servings[g])\n",
    "\n",
    "score_cols = [g + \"_score\" for g in healthy_groups + unhealthy_groups]\n",
    "\n",
    "servings[\"rpdqs_total\"] = servings[score_cols].sum(axis=1)\n",
    "\n",
    "# There are 14 groups * max 4 points = 56\n",
    "servings[\"rpdqs_normalized\"] = (servings[\"rpdqs_total\"] / 56.0) * 100\n",
    "\n",
    "# Merge back into main df\n",
    "df = df.merge(\n",
    "    servings[[\"rpdqs_total\",\"rpdqs_normalized\"]],\n",
    "    left_on=\"SEQN\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# This is now your fully-featured dataset\n",
    "df_final = df.copy()\n",
    "print(\"\\n✓ rPDQS columns present?\",\n",
    "      \"rpdqs_total\" in df_final.columns,\n",
    "      \"rpdqs_normalized\" in df_final.columns)\n",
    "print(df_final[[\"SEQN\",\"rpdqs_total\",\"rpdqs_normalized\"]].head())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Longevity Score v1.1: normalize all inputs (0–100)\n",
    "# ============================================================\n",
    "\n",
    "df = df_final.copy()\n",
    "\n",
    "# ---- BMI ----\n",
    "if {\"BMXWT\", \"BMXHT\"}.issubset(df.columns):\n",
    "    df[\"bmi\"] = df[\"BMXWT\"] / (df[\"BMXHT\"] / 100)**2\n",
    "else:\n",
    "    df[\"bmi\"] = np.nan\n",
    "\n",
    "def normalize_positive(x, low, high):\n",
    "    return ((x - low) / (high - low)) * 100\n",
    "\n",
    "def normalize_negative(x, low, high):\n",
    "    return ((high - x) / (high - low)) * 100\n",
    "\n",
    "# VO2max & OGTT not in NHANES\n",
    "df[\"vo2max_raw\"] = np.nan\n",
    "df[\"vo2max_norm\"] = np.nan\n",
    "df[\"ogtt_raw\"] = np.nan\n",
    "df[\"ogtt_norm\"] = np.nan\n",
    "\n",
    "# ApoB\n",
    "if \"apob_est\" in df.columns:\n",
    "    df[\"apob_norm\"] = normalize_negative(df[\"apob_est\"], low=40, high=120)\n",
    "else:\n",
    "    df[\"apob_norm\"] = np.nan\n",
    "\n",
    "# CRP (LBXCRP)\n",
    "if \"LBXCRP\" in df.columns:\n",
    "    df[\"crp_norm\"] = normalize_negative(df[\"LBXCRP\"], low=0.1, high=10)\n",
    "else:\n",
    "    df[\"crp_norm\"] = np.nan\n",
    "\n",
    "# BMI\n",
    "df[\"bmi_norm\"] = normalize_negative(df[\"bmi\"], low=18.5, high=35)\n",
    "\n",
    "# Smoking pack-years (very crude proxy)\n",
    "if {\"SMD630\", \"SMD641\"}.issubset(df.columns):\n",
    "    df[\"cigs_per_day\"] = df[\"SMD630\"]\n",
    "    df[\"years_smoked\"] = df[\"SMD641\"]\n",
    "    df[\"pack_years\"] = (df[\"cigs_per_day\"] / 20) * df[\"years_smoked\"]\n",
    "    df[\"pack_years_norm\"] = normalize_negative(df[\"pack_years\"], low=0, high=50)\n",
    "else:\n",
    "    df[\"pack_years_norm\"] = np.nan\n",
    "\n",
    "# MoCA not in NHANES\n",
    "df[\"moca_norm\"] = np.nan\n",
    "\n",
    "# MVPA\n",
    "if \"mvpa_min_week\" in df.columns:\n",
    "    df[\"mvpa_norm\"] = normalize_positive(df[\"mvpa_min_week\"], low=0, high=300)\n",
    "else:\n",
    "    df[\"mvpa_norm\"] = np.nan\n",
    "\n",
    "# CAC / HRV not in NHANES\n",
    "df[\"cac_norm\"] = np.nan\n",
    "df[\"hrv_norm\"] = np.nan\n",
    "\n",
    "# PHQ-9\n",
    "df[\"phq9_norm\"] = normalize_negative(df[\"phq9_total\"], low=0, high=27)\n",
    "\n",
    "# ALT (LBXSGPT)\n",
    "if \"LBXSGPT\" in df.columns:\n",
    "    df[\"alt_norm\"] = normalize_negative(df[\"LBXSGPT\"], low=8, high=50)\n",
    "else:\n",
    "    df[\"alt_norm\"] = np.nan\n",
    "\n",
    "# eGFR\n",
    "if \"egfr\" in df.columns:\n",
    "    df[\"egfr_norm\"] = normalize_positive(df[\"egfr\"], low=60, high=110)\n",
    "else:\n",
    "    df[\"egfr_norm\"] = np.nan\n",
    "\n",
    "# BMD, TruAge, shdl, REM, SWLS not available\n",
    "df[\"bmd_norm\"] = np.nan\n",
    "df[\"truage_norm\"] = np.nan\n",
    "df[\"shdl_norm\"] = np.nan\n",
    "df[\"rem_norm\"] = np.nan\n",
    "df[\"swls_norm\"] = np.nan\n",
    "\n",
    "# Grip strength (if MGDC* present)\n",
    "grip_vars = [c for c in df.columns if \"MGDC\" in c]\n",
    "if grip_vars:\n",
    "    df[\"grip_strength\"] = df[grip_vars].max(axis=1)\n",
    "    df[\"grip_norm\"] = normalize_positive(df[\"grip_strength\"], low=10, high=60)\n",
    "else:\n",
    "    df[\"grip_norm\"] = np.nan\n",
    "\n",
    "# Diet quality\n",
    "df[\"diet_norm\"] = df[\"rpdqs_normalized\"]\n",
    "\n",
    "# Collect Longevity Score 1.1 variables\n",
    "longevity_inputs = [\n",
    "    \"ogtt_norm\",\"apob_norm\",\"vo2max_norm\",\"crp_norm\",\"bmi_norm\",\n",
    "    \"pack_years_norm\",\"moca_norm\",\"mvpa_norm\",\"cac_norm\",\"hrv_norm\",\n",
    "    \"phq9_norm\",\"alt_norm\",\"egfr_norm\",\"bmd_norm\",\"truage_norm\",\n",
    "    \"shdl_norm\",\"rem_norm\",\"grip_norm\",\"swls_norm\",\"diet_norm\"\n",
    "]\n",
    "\n",
    "df_longevity = df[[\"SEQN\"] + longevity_inputs].copy()\n",
    "\n",
    "print(\"\\nLongevity variable matrix shape:\", df_longevity.shape)\n",
    "print(df_longevity.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f7b1b6-7a17-47ed-ae4f-ab1efc049c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_nhanes_merge(df_master, fped):\n",
    "    print(\"===============================================\")\n",
    "    print(\"        NHANES MERGE QUALITY CHECK (QC)\")\n",
    "    print(\"===============================================\\n\")\n",
    "\n",
    "    # 1. Shape\n",
    "    print(\"[1] Dataset shape:\")\n",
    "    print(f\"    df_master.shape = {df_master.shape}\")\n",
    "    if df_master.shape[0] == 9971:\n",
    "        print(\"    ✓ PASS — correct number of rows for 2015–2016 DEMO\\n\")\n",
    "    else:\n",
    "        print(\"    ✗ WARN — unexpected number of rows\\n\")\n",
    "\n",
    "    # 2. SEQN uniqueness\n",
    "    print(\"[2] Unique SEQN check:\")\n",
    "    is_unique = df_master[\"SEQN\"].is_unique\n",
    "    print(f\"    SEQN unique? → {is_unique}\")\n",
    "    print(\"    \" + (\"✓ PASS\\n\" if is_unique else \"✗ FAIL — duplicate SEQN values\\n\"))\n",
    "\n",
    "    # 3. Any leftover _x/_y columns?\n",
    "    print(\"[3] Duplicate merge columns (_x/_y):\")\n",
    "    dup_cols = [c for c in df_master.columns if c.endswith(\"_x\") or c.endswith(\"_y\")]\n",
    "    print(f\"    Found: {dup_cols}\")\n",
    "    if len(dup_cols) == 0:\n",
    "        print(\"    ✓ PASS — all merge duplicates cleaned\\n\")\n",
    "    else:\n",
    "        print(\"    ✗ FAIL — some *_x / *_y columns still present\\n\")\n",
    "\n",
    "    # 4. FPED variable check\n",
    "    print(\"[4] FPED variable check:\")\n",
    "    # How many DR1T_* variables exist in the original FPED file?\n",
    "    fped_drs = [c for c in fped.columns if c.startswith(\"DR1T_\")]\n",
    "    df_drs   = [c for c in df_master.columns if c.startswith(\"DR1T_\")]\n",
    "    print(f\"    Number of DR1T_* in FPED:      {len(fped_drs)}\")\n",
    "    print(f\"    Number of DR1T_* in df_master: {len(df_drs)}\")\n",
    "\n",
    "    if set(fped_drs).issubset(df_master.columns):\n",
    "        print(\"    ✓ PASS — all FPED DR1T_* vars are present in df_master\\n\")\n",
    "    else:\n",
    "        missing = [c for c in fped_drs if c not in df_master.columns]\n",
    "        print(\"    ✗ FAIL — some FPED DR1T_* vars missing from df_master:\")\n",
    "        print(f\"        Missing: {missing}\\n\")\n",
    "\n",
    "    # 5. DR1DRSTZ for SEQN 93700\n",
    "    print(\"[5] DR1DRSTZ for SEQN 93700 (expected = 2.0):\")\n",
    "    val = df_master.loc[df_master[\"SEQN\"] == 93700, \"DR1DRSTZ\"].tolist()\n",
    "    print(f\"    Value: {val}\")\n",
    "    if val == [2.0] or val == [2]:\n",
    "        print(\"    ✓ PASS — correct DR1DRSTZ merge for 93700\\n\")\n",
    "    else:\n",
    "        print(\"    ✗ WARN — unexpected DR1DRSTZ for 93700\\n\")\n",
    "\n",
    "    # 6. Check if 93700 exists in FPED\n",
    "    print(\"[6] Check if 93700 exists in FPED:\")\n",
    "    exists_fped = 93700 in fped[\"SEQN\"].values\n",
    "    print(f\"    Exists in FPED? → {exists_fped}\")\n",
    "    print(\"    \" + (\"✓ PASS — FPED contains 93700\\n\" if exists_fped else \"✗ FAIL — 93700 not found in FPED\\n\"))\n",
    "\n",
    "    print(\"===============================================\")\n",
    "    print(\"QC complete.\")\n",
    "    print(\"===============================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36ff84d-1ff4-4503-9d29-02e79203f86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "        NHANES MERGE QUALITY CHECK (QC)\n",
      "===============================================\n",
      "\n",
      "[1] Dataset shape:\n",
      "    df_master.shape = (9971, 440)\n",
      "    ✓ PASS — correct number of rows for 2015–2016 DEMO\n",
      "\n",
      "[2] Unique SEQN check:\n",
      "    SEQN unique? → True\n",
      "    ✓ PASS\n",
      "\n",
      "[3] Duplicate merge columns (_x/_y):\n",
      "    Found: []\n",
      "    ✓ PASS — all merge duplicates cleaned\n",
      "\n",
      "[4] FPED variable check:\n",
      "    Number of DR1T_* in FPED:      37\n",
      "    Number of DR1T_* in df_master: 37\n",
      "    ✓ PASS — all FPED DR1T_* vars are present in df_master\n",
      "\n",
      "[5] DR1DRSTZ for SEQN 93700 (expected = 2.0):\n",
      "    Value: [2.0]\n",
      "    ✓ PASS — correct DR1DRSTZ merge for 93700\n",
      "\n",
      "[6] Check if 93700 exists in FPED:\n",
      "    Exists in FPED? → True\n",
      "    ✓ PASS — FPED contains 93700\n",
      "\n",
      "===============================================\n",
      "QC complete.\n",
      "===============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qc_nhanes_merge(df_master, fped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153d184-6563-4246-b6e9-9780e86caef3",
   "metadata": {},
   "source": [
    "# optional code to examine full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f2960d-a34f-4785-accd-f1267dbd0886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 440\n",
      "\n",
      "--- FULL VARIABLE LIST ---\n",
      "SEQN\n",
      "SDDSRVYR\n",
      "RIDSTATR\n",
      "RIAGENDR\n",
      "RIDAGEYR\n",
      "RIDAGEMN\n",
      "RIDRETH1\n",
      "RIDRETH3\n",
      "RIDEXMON\n",
      "RIDEXAGM\n",
      "DMQMILIZ\n",
      "DMQADFC\n",
      "DMDBORN4\n",
      "DMDCITZN\n",
      "DMDYRSUS\n",
      "DMDEDUC3\n",
      "DMDEDUC2\n",
      "DMDMARTL\n",
      "RIDEXPRG\n",
      "SIALANG\n",
      "SIAPROXY\n",
      "SIAINTRP\n",
      "FIALANG\n",
      "FIAPROXY\n",
      "FIAINTRP\n",
      "MIALANG\n",
      "MIAPROXY\n",
      "MIAINTRP\n",
      "AIALANGA\n",
      "DMDHHSIZ\n",
      "DMDFMSIZ\n",
      "DMDHHSZA\n",
      "DMDHHSZB\n",
      "DMDHHSZE\n",
      "DMDHRGND\n",
      "DMDHRAGE\n",
      "DMDHRBR4\n",
      "DMDHREDU\n",
      "DMDHRMAR\n",
      "DMDHSEDU\n",
      "WTINT2YR\n",
      "WTMEC2YR\n",
      "SDMVPSU\n",
      "SDMVSTRA\n",
      "INDHHIN2\n",
      "INDFMIN2\n",
      "INDFMPIR\n",
      "LBDHDD\n",
      "LBDHDDSI\n",
      "LBXTC\n",
      "LBDTCSI\n",
      "LBXTR\n",
      "LBDTRSI\n",
      "LBDLDL\n",
      "LBDLDLSI\n",
      "LBXGLU\n",
      "LBDGLUSI\n",
      "LBXIN\n",
      "LBDINSI\n",
      "LBDINLC\n",
      "PHAFSTHR\n",
      "PHAFSTMN\n",
      "DPQ010\n",
      "DPQ020\n",
      "DPQ030\n",
      "DPQ040\n",
      "DPQ050\n",
      "DPQ060\n",
      "DPQ070\n",
      "DPQ080\n",
      "DPQ090\n",
      "DPQ100\n",
      "SLQ300\n",
      "SLQ310\n",
      "SLD012\n",
      "SLQ030\n",
      "SLQ040\n",
      "SLQ050\n",
      "SLQ120\n",
      "WTDRD1\n",
      "WTDR2D\n",
      "DR1DRSTZ\n",
      "DR1EXMER\n",
      "DRABF\n",
      "DRDINT\n",
      "DR1DBIH\n",
      "DR1DAY\n",
      "DR1LANG\n",
      "DR1MRESP\n",
      "DR1HELP\n",
      "DBQ095Z\n",
      "DBD100\n",
      "DRQSPREP\n",
      "DR1STY\n",
      "DR1SKY\n",
      "DRQSDIET\n",
      "DRQSDT1\n",
      "DRQSDT2\n",
      "DRQSDT3\n",
      "DRQSDT4\n",
      "DRQSDT5\n",
      "DRQSDT6\n",
      "DRQSDT7\n",
      "DRQSDT8\n",
      "DRQSDT9\n",
      "DRQSDT10\n",
      "DRQSDT11\n",
      "DRQSDT12\n",
      "DRQSDT91\n",
      "DR1TNUMF\n",
      "DR1TKCAL\n",
      "DR1TPROT\n",
      "DR1TCARB\n",
      "DR1TSUGR\n",
      "DR1TFIBE\n",
      "DR1TTFAT\n",
      "DR1TSFAT\n",
      "DR1TMFAT\n",
      "DR1TPFAT\n",
      "DR1TCHOL\n",
      "DR1TATOC\n",
      "DR1TATOA\n",
      "DR1TRET\n",
      "DR1TVARA\n",
      "DR1TACAR\n",
      "DR1TBCAR\n",
      "DR1TCRYP\n",
      "DR1TLYCO\n",
      "DR1TLZ\n",
      "DR1TVB1\n",
      "DR1TVB2\n",
      "DR1TNIAC\n",
      "DR1TVB6\n",
      "DR1TFOLA\n",
      "DR1TFA\n",
      "DR1TFF\n",
      "DR1TFDFE\n",
      "DR1TCHL\n",
      "DR1TVB12\n",
      "DR1TB12A\n",
      "DR1TVC\n",
      "DR1TVD\n",
      "DR1TVK\n",
      "DR1TCALC\n",
      "DR1TPHOS\n",
      "DR1TMAGN\n",
      "DR1TIRON\n",
      "DR1TZINC\n",
      "DR1TCOPP\n",
      "DR1TSODI\n",
      "DR1TPOTA\n",
      "DR1TSELE\n",
      "DR1TCAFF\n",
      "DR1TTHEO\n",
      "DR1TALCO\n",
      "DR1TMOIS\n",
      "DR1TS040\n",
      "DR1TS060\n",
      "DR1TS080\n",
      "DR1TS100\n",
      "DR1TS120\n",
      "DR1TS140\n",
      "DR1TS160\n",
      "DR1TS180\n",
      "DR1TM161\n",
      "DR1TM181\n",
      "DR1TM201\n",
      "DR1TM221\n",
      "DR1TP182\n",
      "DR1TP183\n",
      "DR1TP184\n",
      "DR1TP204\n",
      "DR1TP205\n",
      "DR1TP225\n",
      "DR1TP226\n",
      "DR1_300\n",
      "DR1_320Z\n",
      "DR1_330Z\n",
      "DR1BWATZ\n",
      "DR1TWS\n",
      "DRD340\n",
      "DRD350A\n",
      "DRD350AQ\n",
      "DRD350B\n",
      "DRD350BQ\n",
      "DRD350C\n",
      "DRD350CQ\n",
      "DRD350D\n",
      "DRD350DQ\n",
      "DRD350E\n",
      "DRD350EQ\n",
      "DRD350F\n",
      "DRD350FQ\n",
      "DRD350G\n",
      "DRD350GQ\n",
      "DRD350H\n",
      "DRD350HQ\n",
      "DRD350I\n",
      "DRD350IQ\n",
      "DRD350J\n",
      "DRD350JQ\n",
      "DRD350K\n",
      "DRD360\n",
      "DRD370A\n",
      "DRD370AQ\n",
      "DRD370B\n",
      "DRD370BQ\n",
      "DRD370C\n",
      "DRD370CQ\n",
      "DRD370D\n",
      "DRD370DQ\n",
      "DRD370E\n",
      "DRD370EQ\n",
      "DRD370F\n",
      "DRD370FQ\n",
      "DRD370G\n",
      "DRD370GQ\n",
      "DRD370H\n",
      "DRD370HQ\n",
      "DRD370I\n",
      "DRD370IQ\n",
      "DRD370J\n",
      "DRD370JQ\n",
      "DRD370K\n",
      "DRD370KQ\n",
      "DRD370L\n",
      "DRD370LQ\n",
      "DRD370M\n",
      "DRD370MQ\n",
      "DRD370N\n",
      "DRD370NQ\n",
      "DRD370O\n",
      "DRD370OQ\n",
      "DRD370P\n",
      "DRD370PQ\n",
      "DRD370Q\n",
      "DRD370QQ\n",
      "DRD370R\n",
      "DRD370RQ\n",
      "DRD370S\n",
      "DRD370SQ\n",
      "DRD370T\n",
      "DRD370TQ\n",
      "DRD370U\n",
      "DRD370UQ\n",
      "DRD370V\n",
      "PAQ605\n",
      "PAQ610\n",
      "PAD615\n",
      "PAQ620\n",
      "PAQ625\n",
      "PAD630\n",
      "PAQ635\n",
      "PAQ640\n",
      "PAD645\n",
      "PAQ650\n",
      "PAQ655\n",
      "PAD660\n",
      "PAQ665\n",
      "PAQ670\n",
      "PAD675\n",
      "PAD680\n",
      "PAQ706\n",
      "PAQ710\n",
      "PAQ715\n",
      "PAQ722\n",
      "PAQ724A\n",
      "PAQ724B\n",
      "PAQ724C\n",
      "PAQ724D\n",
      "PAQ724E\n",
      "PAQ724F\n",
      "PAQ724G\n",
      "PAQ724H\n",
      "PAQ724I\n",
      "PAQ724J\n",
      "PAQ724K\n",
      "PAQ724L\n",
      "PAQ724M\n",
      "PAQ724N\n",
      "PAQ724O\n",
      "PAQ724P\n",
      "PAQ724Q\n",
      "PAQ724R\n",
      "PAQ724S\n",
      "PAQ724T\n",
      "PAQ724U\n",
      "PAQ724V\n",
      "PAQ724W\n",
      "PAQ724X\n",
      "PAQ724Y\n",
      "PAQ724Z\n",
      "PAQ724AA\n",
      "PAQ724AB\n",
      "PAQ724AC\n",
      "PAQ724AD\n",
      "PAQ724AE\n",
      "PAQ724AF\n",
      "PAQ724CM\n",
      "PAQ731\n",
      "PAD733\n",
      "PAQ677\n",
      "PAQ678\n",
      "PAQ740\n",
      "PAQ742\n",
      "PAQ744\n",
      "PAQ746\n",
      "PAQ748\n",
      "PAQ755\n",
      "PAQ759A\n",
      "PAQ759B\n",
      "PAQ759C\n",
      "PAQ759D\n",
      "PAQ759E\n",
      "PAQ759F\n",
      "PAQ759G\n",
      "PAQ759H\n",
      "PAQ759I\n",
      "PAQ759J\n",
      "PAQ759K\n",
      "PAQ759L\n",
      "PAQ759M\n",
      "PAQ759N\n",
      "PAQ759O\n",
      "PAQ759P\n",
      "PAQ759Q\n",
      "PAQ759R\n",
      "PAQ759S\n",
      "PAQ759T\n",
      "PAQ759U\n",
      "PAQ762\n",
      "PAQ764\n",
      "PAQ766\n",
      "PAQ679\n",
      "PAQ750\n",
      "PAQ770\n",
      "PAQ772A\n",
      "PAQ772B\n",
      "PAQ772C\n",
      "PEASCCT1\n",
      "BPXCHR\n",
      "BPAARM\n",
      "BPACSZ\n",
      "BPXPLS\n",
      "BPXPULS\n",
      "BPXPTY\n",
      "BPXML1\n",
      "BPXSY1\n",
      "BPXDI1\n",
      "BPAEN1\n",
      "BPXSY2\n",
      "BPXDI2\n",
      "BPAEN2\n",
      "BPXSY3\n",
      "BPXDI3\n",
      "BPAEN3\n",
      "BPXSY4\n",
      "BPXDI4\n",
      "BPAEN4\n",
      "LBXSAL\n",
      "LBDSALSI\n",
      "LBXSAPSI\n",
      "LBXSASSI\n",
      "LBXSATSI\n",
      "LBXSBU\n",
      "LBDSBUSI\n",
      "LBXSC3SI\n",
      "LBXSCA\n",
      "LBDSCASI\n",
      "LBXSCH\n",
      "LBDSCHSI\n",
      "LBXSCK\n",
      "LBXSCLSI\n",
      "LBXSCR\n",
      "LBDSCRSI\n",
      "LBXSGB\n",
      "LBDSGBSI\n",
      "LBXSGL\n",
      "LBDSGLSI\n",
      "LBXSGTSI\n",
      "LBXSIR\n",
      "LBDSIRSI\n",
      "LBXSKSI\n",
      "LBXSLDSI\n",
      "LBXSNASI\n",
      "LBXSOSSI\n",
      "LBXSPH\n",
      "LBDSPHSI\n",
      "LBXSTB\n",
      "LBDSTBSI\n",
      "LBXSTP\n",
      "LBDSTPSI\n",
      "LBXSTR\n",
      "LBDSTRSI\n",
      "LBXSUA\n",
      "LBDSUASI\n",
      "URXUMA\n",
      "URDUMALC\n",
      "URXUMS\n",
      "URXUCR\n",
      "URDUCRLC\n",
      "URXCRS\n",
      "URDACT\n",
      "DR1T_F_CITMLB\n",
      "DR1T_F_OTHER\n",
      "DR1T_F_JUICE\n",
      "DR1T_F_TOTAL\n",
      "DR1T_V_DRKGR\n",
      "DR1T_V_REDOR_TOMATO\n",
      "DR1T_V_REDOR_OTHER\n",
      "DR1T_V_REDOR_TOTAL\n",
      "DR1T_V_STARCHY_POTATO\n",
      "DR1T_V_STARCHY_OTHER\n",
      "DR1T_V_STARCHY_TOTAL\n",
      "DR1T_V_OTHER\n",
      "DR1T_V_TOTAL\n",
      "DR1T_V_LEGUMES\n",
      "DR1T_G_WHOLE\n",
      "DR1T_G_REFINED\n",
      "DR1T_G_TOTAL\n",
      "DR1T_PF_MEAT\n",
      "DR1T_PF_CUREDMEAT\n",
      "DR1T_PF_ORGAN\n",
      "DR1T_PF_POULT\n",
      "DR1T_PF_SEAFD_HI\n",
      "DR1T_PF_SEAFD_LOW\n",
      "DR1T_PF_MPS_TOTAL\n",
      "DR1T_PF_EGGS\n",
      "DR1T_PF_SOY\n",
      "DR1T_PF_NUTSDS\n",
      "DR1T_PF_LEGUMES\n",
      "DR1T_PF_TOTAL\n",
      "DR1T_D_MILK\n",
      "DR1T_D_YOGURT\n",
      "DR1T_D_CHEESE\n",
      "DR1T_D_TOTAL\n",
      "DR1T_OILS\n",
      "DR1T_SOLID_FATS\n",
      "DR1T_ADD_SUGARS\n",
      "DR1T_A_DRINKS\n",
      "\n",
      "--- DATA TYPES ---\n",
      "SEQN               float64\n",
      "SDDSRVYR           float64\n",
      "RIDSTATR           float64\n",
      "RIAGENDR           float64\n",
      "RIDAGEYR           float64\n",
      "                    ...   \n",
      "DR1T_D_TOTAL       float64\n",
      "DR1T_OILS          float64\n",
      "DR1T_SOLID_FATS    float64\n",
      "DR1T_ADD_SUGARS    float64\n",
      "DR1T_A_DRINKS      float64\n",
      "Length: 440, dtype: object\n",
      "\n",
      "--- VARIABLE PREFIX COUNTS ---\n",
      "[('DR1T', 37), ('DR1', 3), ('SEQN', 1), ('SDDSRVYR', 1), ('RIDSTATR', 1), ('RIAGENDR', 1), ('RIDAGEYR', 1), ('RIDAGEMN', 1), ('RIDRETH1', 1), ('RIDRETH3', 1), ('RIDEXMON', 1), ('RIDEXAGM', 1), ('DMQMILIZ', 1), ('DMQADFC', 1), ('DMDBORN4', 1), ('DMDCITZN', 1), ('DMDYRSUS', 1), ('DMDEDUC3', 1), ('DMDEDUC2', 1), ('DMDMARTL', 1), ('RIDEXPRG', 1), ('SIALANG', 1), ('SIAPROXY', 1), ('SIAINTRP', 1), ('FIALANG', 1), ('FIAPROXY', 1), ('FIAINTRP', 1), ('MIALANG', 1), ('MIAPROXY', 1), ('MIAINTRP', 1), ('AIALANGA', 1), ('DMDHHSIZ', 1), ('DMDFMSIZ', 1), ('DMDHHSZA', 1), ('DMDHHSZB', 1), ('DMDHHSZE', 1), ('DMDHRGND', 1), ('DMDHRAGE', 1), ('DMDHRBR4', 1), ('DMDHREDU', 1), ('DMDHRMAR', 1), ('DMDHSEDU', 1), ('WTINT2YR', 1), ('WTMEC2YR', 1), ('SDMVPSU', 1), ('SDMVSTRA', 1), ('INDHHIN2', 1), ('INDFMIN2', 1), ('INDFMPIR', 1), ('LBDHDD', 1), ('LBDHDDSI', 1), ('LBXTC', 1), ('LBDTCSI', 1), ('LBXTR', 1), ('LBDTRSI', 1), ('LBDLDL', 1), ('LBDLDLSI', 1), ('LBXGLU', 1), ('LBDGLUSI', 1), ('LBXIN', 1), ('LBDINSI', 1), ('LBDINLC', 1), ('PHAFSTHR', 1), ('PHAFSTMN', 1), ('DPQ010', 1), ('DPQ020', 1), ('DPQ030', 1), ('DPQ040', 1), ('DPQ050', 1), ('DPQ060', 1), ('DPQ070', 1), ('DPQ080', 1), ('DPQ090', 1), ('DPQ100', 1), ('SLQ300', 1), ('SLQ310', 1), ('SLD012', 1), ('SLQ030', 1), ('SLQ040', 1), ('SLQ050', 1), ('SLQ120', 1), ('WTDRD1', 1), ('WTDR2D', 1), ('DR1DRSTZ', 1), ('DR1EXMER', 1), ('DRABF', 1), ('DRDINT', 1), ('DR1DBIH', 1), ('DR1DAY', 1), ('DR1LANG', 1), ('DR1MRESP', 1), ('DR1HELP', 1), ('DBQ095Z', 1), ('DBD100', 1), ('DRQSPREP', 1), ('DR1STY', 1), ('DR1SKY', 1), ('DRQSDIET', 1), ('DRQSDT1', 1), ('DRQSDT2', 1), ('DRQSDT3', 1), ('DRQSDT4', 1), ('DRQSDT5', 1), ('DRQSDT6', 1), ('DRQSDT7', 1), ('DRQSDT8', 1), ('DRQSDT9', 1), ('DRQSDT10', 1), ('DRQSDT11', 1), ('DRQSDT12', 1), ('DRQSDT91', 1), ('DR1TNUMF', 1), ('DR1TKCAL', 1), ('DR1TPROT', 1), ('DR1TCARB', 1), ('DR1TSUGR', 1), ('DR1TFIBE', 1), ('DR1TTFAT', 1), ('DR1TSFAT', 1), ('DR1TMFAT', 1), ('DR1TPFAT', 1), ('DR1TCHOL', 1), ('DR1TATOC', 1), ('DR1TATOA', 1), ('DR1TRET', 1), ('DR1TVARA', 1), ('DR1TACAR', 1), ('DR1TBCAR', 1), ('DR1TCRYP', 1), ('DR1TLYCO', 1), ('DR1TLZ', 1), ('DR1TVB1', 1), ('DR1TVB2', 1), ('DR1TNIAC', 1), ('DR1TVB6', 1), ('DR1TFOLA', 1), ('DR1TFA', 1), ('DR1TFF', 1), ('DR1TFDFE', 1), ('DR1TCHL', 1), ('DR1TVB12', 1), ('DR1TB12A', 1), ('DR1TVC', 1), ('DR1TVD', 1), ('DR1TVK', 1), ('DR1TCALC', 1), ('DR1TPHOS', 1), ('DR1TMAGN', 1), ('DR1TIRON', 1), ('DR1TZINC', 1), ('DR1TCOPP', 1), ('DR1TSODI', 1), ('DR1TPOTA', 1), ('DR1TSELE', 1), ('DR1TCAFF', 1), ('DR1TTHEO', 1), ('DR1TALCO', 1), ('DR1TMOIS', 1), ('DR1TS040', 1), ('DR1TS060', 1), ('DR1TS080', 1), ('DR1TS100', 1), ('DR1TS120', 1), ('DR1TS140', 1), ('DR1TS160', 1), ('DR1TS180', 1), ('DR1TM161', 1), ('DR1TM181', 1), ('DR1TM201', 1), ('DR1TM221', 1), ('DR1TP182', 1), ('DR1TP183', 1), ('DR1TP184', 1), ('DR1TP204', 1), ('DR1TP205', 1), ('DR1TP225', 1), ('DR1TP226', 1), ('DR1BWATZ', 1), ('DR1TWS', 1), ('DRD340', 1), ('DRD350A', 1), ('DRD350AQ', 1), ('DRD350B', 1), ('DRD350BQ', 1), ('DRD350C', 1), ('DRD350CQ', 1), ('DRD350D', 1), ('DRD350DQ', 1), ('DRD350E', 1), ('DRD350EQ', 1), ('DRD350F', 1), ('DRD350FQ', 1), ('DRD350G', 1), ('DRD350GQ', 1), ('DRD350H', 1), ('DRD350HQ', 1), ('DRD350I', 1), ('DRD350IQ', 1), ('DRD350J', 1), ('DRD350JQ', 1), ('DRD350K', 1), ('DRD360', 1), ('DRD370A', 1), ('DRD370AQ', 1), ('DRD370B', 1), ('DRD370BQ', 1), ('DRD370C', 1), ('DRD370CQ', 1), ('DRD370D', 1), ('DRD370DQ', 1), ('DRD370E', 1), ('DRD370EQ', 1), ('DRD370F', 1), ('DRD370FQ', 1), ('DRD370G', 1), ('DRD370GQ', 1), ('DRD370H', 1), ('DRD370HQ', 1), ('DRD370I', 1), ('DRD370IQ', 1), ('DRD370J', 1), ('DRD370JQ', 1), ('DRD370K', 1), ('DRD370KQ', 1), ('DRD370L', 1), ('DRD370LQ', 1), ('DRD370M', 1), ('DRD370MQ', 1), ('DRD370N', 1), ('DRD370NQ', 1), ('DRD370O', 1), ('DRD370OQ', 1), ('DRD370P', 1), ('DRD370PQ', 1), ('DRD370Q', 1), ('DRD370QQ', 1), ('DRD370R', 1), ('DRD370RQ', 1), ('DRD370S', 1), ('DRD370SQ', 1), ('DRD370T', 1), ('DRD370TQ', 1), ('DRD370U', 1), ('DRD370UQ', 1), ('DRD370V', 1), ('PAQ605', 1), ('PAQ610', 1), ('PAD615', 1), ('PAQ620', 1), ('PAQ625', 1), ('PAD630', 1), ('PAQ635', 1), ('PAQ640', 1), ('PAD645', 1), ('PAQ650', 1), ('PAQ655', 1), ('PAD660', 1), ('PAQ665', 1), ('PAQ670', 1), ('PAD675', 1), ('PAD680', 1), ('PAQ706', 1), ('PAQ710', 1), ('PAQ715', 1), ('PAQ722', 1), ('PAQ724A', 1), ('PAQ724B', 1), ('PAQ724C', 1), ('PAQ724D', 1), ('PAQ724E', 1), ('PAQ724F', 1), ('PAQ724G', 1), ('PAQ724H', 1), ('PAQ724I', 1), ('PAQ724J', 1), ('PAQ724K', 1), ('PAQ724L', 1), ('PAQ724M', 1), ('PAQ724N', 1), ('PAQ724O', 1), ('PAQ724P', 1), ('PAQ724Q', 1), ('PAQ724R', 1), ('PAQ724S', 1), ('PAQ724T', 1), ('PAQ724U', 1), ('PAQ724V', 1), ('PAQ724W', 1), ('PAQ724X', 1), ('PAQ724Y', 1), ('PAQ724Z', 1), ('PAQ724AA', 1), ('PAQ724AB', 1), ('PAQ724AC', 1), ('PAQ724AD', 1), ('PAQ724AE', 1), ('PAQ724AF', 1), ('PAQ724CM', 1), ('PAQ731', 1), ('PAD733', 1), ('PAQ677', 1), ('PAQ678', 1), ('PAQ740', 1), ('PAQ742', 1), ('PAQ744', 1), ('PAQ746', 1), ('PAQ748', 1), ('PAQ755', 1), ('PAQ759A', 1), ('PAQ759B', 1), ('PAQ759C', 1), ('PAQ759D', 1), ('PAQ759E', 1), ('PAQ759F', 1), ('PAQ759G', 1), ('PAQ759H', 1), ('PAQ759I', 1), ('PAQ759J', 1), ('PAQ759K', 1), ('PAQ759L', 1), ('PAQ759M', 1), ('PAQ759N', 1), ('PAQ759O', 1), ('PAQ759P', 1), ('PAQ759Q', 1), ('PAQ759R', 1), ('PAQ759S', 1), ('PAQ759T', 1), ('PAQ759U', 1), ('PAQ762', 1), ('PAQ764', 1), ('PAQ766', 1), ('PAQ679', 1), ('PAQ750', 1), ('PAQ770', 1), ('PAQ772A', 1), ('PAQ772B', 1), ('PAQ772C', 1), ('PEASCCT1', 1), ('BPXCHR', 1), ('BPAARM', 1), ('BPACSZ', 1), ('BPXPLS', 1), ('BPXPULS', 1), ('BPXPTY', 1), ('BPXML1', 1), ('BPXSY1', 1), ('BPXDI1', 1), ('BPAEN1', 1), ('BPXSY2', 1), ('BPXDI2', 1), ('BPAEN2', 1), ('BPXSY3', 1), ('BPXDI3', 1), ('BPAEN3', 1), ('BPXSY4', 1), ('BPXDI4', 1), ('BPAEN4', 1), ('LBXSAL', 1), ('LBDSALSI', 1), ('LBXSAPSI', 1), ('LBXSASSI', 1), ('LBXSATSI', 1), ('LBXSBU', 1), ('LBDSBUSI', 1), ('LBXSC3SI', 1), ('LBXSCA', 1), ('LBDSCASI', 1), ('LBXSCH', 1), ('LBDSCHSI', 1), ('LBXSCK', 1), ('LBXSCLSI', 1), ('LBXSCR', 1), ('LBDSCRSI', 1), ('LBXSGB', 1), ('LBDSGBSI', 1), ('LBXSGL', 1), ('LBDSGLSI', 1), ('LBXSGTSI', 1), ('LBXSIR', 1), ('LBDSIRSI', 1), ('LBXSKSI', 1), ('LBXSLDSI', 1), ('LBXSNASI', 1), ('LBXSOSSI', 1), ('LBXSPH', 1), ('LBDSPHSI', 1), ('LBXSTB', 1), ('LBDSTBSI', 1), ('LBXSTP', 1), ('LBDSTPSI', 1), ('LBXSTR', 1), ('LBDSTRSI', 1), ('LBXSUA', 1), ('LBDSUASI', 1), ('URXUMA', 1), ('URDUMALC', 1), ('URXUMS', 1), ('URXUCR', 1), ('URDUCRLC', 1), ('URXCRS', 1), ('URDACT', 1)]\n",
      "\n",
      "--- FIRST 5 ROWS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>RIDSTATR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>RIDAGEMN</th>\n",
       "      <th>RIDRETH1</th>\n",
       "      <th>RIDRETH3</th>\n",
       "      <th>RIDEXMON</th>\n",
       "      <th>RIDEXAGM</th>\n",
       "      <th>...</th>\n",
       "      <th>DR1T_PF_LEGUMES</th>\n",
       "      <th>DR1T_PF_TOTAL</th>\n",
       "      <th>DR1T_D_MILK</th>\n",
       "      <th>DR1T_D_YOGURT</th>\n",
       "      <th>DR1T_D_CHEESE</th>\n",
       "      <th>DR1T_D_TOTAL</th>\n",
       "      <th>DR1T_OILS</th>\n",
       "      <th>DR1T_SOLID_FATS</th>\n",
       "      <th>DR1T_ADD_SUGARS</th>\n",
       "      <th>DR1T_A_DRINKS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83732.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.69</td>\n",
       "      <td>7.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.93</td>\n",
       "      <td>35.37</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83733.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>4.69</td>\n",
       "      <td>57.97</td>\n",
       "      <td>40.17</td>\n",
       "      <td>6.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83734.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.89</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>37.43</td>\n",
       "      <td>31.27</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83735.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.83</td>\n",
       "      <td>21.12</td>\n",
       "      <td>18.58</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83736.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.40</td>\n",
       "      <td>7.07</td>\n",
       "      <td>13.12</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 440 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDAGEMN  RIDRETH1  \\\n",
       "0  83732.0       9.0       2.0       1.0      62.0       NaN       3.0   \n",
       "1  83733.0       9.0       2.0       1.0      53.0       NaN       3.0   \n",
       "2  83734.0       9.0       2.0       1.0      78.0       NaN       3.0   \n",
       "3  83735.0       9.0       2.0       2.0      56.0       NaN       3.0   \n",
       "4  83736.0       9.0       2.0       2.0      42.0       NaN       4.0   \n",
       "\n",
       "   RIDRETH3  RIDEXMON  RIDEXAGM  ...  DR1T_PF_LEGUMES  DR1T_PF_TOTAL  \\\n",
       "0       3.0       1.0       NaN  ...             2.69           7.57   \n",
       "1       3.0       1.0       NaN  ...             0.00           5.18   \n",
       "2       3.0       2.0       NaN  ...             7.47           7.89   \n",
       "3       3.0       2.0       NaN  ...             0.00           3.33   \n",
       "4       4.0       2.0       NaN  ...             0.00           2.17   \n",
       "\n",
       "   DR1T_D_MILK  DR1T_D_YOGURT  DR1T_D_CHEESE  DR1T_D_TOTAL  DR1T_OILS  \\\n",
       "0         0.00            0.0           0.00          0.00      24.93   \n",
       "1         0.11            0.0           0.00          0.11       4.69   \n",
       "2         0.13            0.0           0.00          0.13      37.43   \n",
       "3         1.66            0.0           1.17          2.83      21.12   \n",
       "4         0.00            0.0           0.00          0.00       6.40   \n",
       "\n",
       "   DR1T_SOLID_FATS  DR1T_ADD_SUGARS  DR1T_A_DRINKS  \n",
       "0            35.37             3.21           0.00  \n",
       "1            57.97            40.17           6.34  \n",
       "2            31.27             3.07           0.00  \n",
       "3            18.58             6.64           0.00  \n",
       "4             7.07            13.12           0.00  \n",
       "\n",
       "[5 rows x 440 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MISSINGNESS (% missing) ---\n",
      "DRQSDT5     0.999799\n",
      "PAQ724M     0.999699\n",
      "PAQ724L     0.999599\n",
      "PAQ724G     0.999499\n",
      "PAQ759C     0.999499\n",
      "              ...   \n",
      "DMDHRGND    0.000000\n",
      "SDMVPSU     0.000000\n",
      "WTMEC2YR    0.000000\n",
      "WTINT2YR    0.000000\n",
      "SDMVSTRA    0.000000\n",
      "Length: 440, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# df_master is the authoritative merged dataset (NHANES + FPED)\n",
    "# Use df_master for all QC, validation, and re-running feature engineering.\n",
    "# df, df_features, df_final, df_longevity are downstream engineered variants.\n",
    "\n",
    "#examine the FULL dataset, df_master\n",
    "\n",
    "print(\"Number of columns:\", df_master.shape[1])\n",
    "\n",
    "print(\"\\n--- FULL VARIABLE LIST ---\")\n",
    "for c in df_master.columns:\n",
    "    print(c)\n",
    "\n",
    "print(\"\\n--- DATA TYPES ---\")\n",
    "print(df_master.dtypes)\n",
    "\n",
    "print(\"\\n--- VARIABLE PREFIX COUNTS ---\")\n",
    "from collections import Counter\n",
    "prefixes = [c.split(\"_\")[0] for c in df_master.columns]\n",
    "print(Counter(prefixes).most_common())\n",
    "\n",
    "print(\"\\n--- FIRST 5 ROWS ---\")\n",
    "display(df_master.head())\n",
    "\n",
    "print(\"\\n--- MISSINGNESS (% missing) ---\")\n",
    "missing = df_master.isna().mean().sort_values(ascending=False)\n",
    "print(missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de6be8-5d85-44aa-964d-0be22c463856",
   "metadata": {},
   "source": [
    "## ignore, this code is for the wrong mortality data set\n",
    "note: it turns out that there is no publically available LMF for this year of NHANES. So this code is pointless\n",
    "because the mortality data is for a totally different year's cohort (I think 2011)\n",
    "I will keep the code here in inactive form as it may prove useful later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0959e46-8402-4f0e-94a5-1fcae8ffefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_longevity.to_csv(\"nhanes_longevity_1516.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f022641-654f-4ddb-acd7-6c6604574bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"nhanes_full_1516.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6ca6f5-02db-4b69-9a2c-a4b3d628df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code imports the mortality LMF file into this data set \n",
    "import pandas as pd\n",
    "\n",
    "df_nhanes = pd.read_csv(\"nhanes_full_1516.csv\")\n",
    "\n",
    "lmf_clean = pd.read_csv(\"nhanes_lmf_clean.csv\")\n",
    "\n",
    "df_merged = df_nhanes.merge(lmf_clean, on=\"SEQN\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a698db-b124-4b13-bffe-666078ae2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: if you want to do the import to the nhanes longevity data set\n",
    "import pandas as pd\n",
    "\n",
    "# df_nhanes = pd.read_csv(\"nhanes_longevity_1516.csv\")\n",
    "\n",
    "# lmf_clean = pd.read_csv(\"nhanes_lmf_clean.csv\")\n",
    "\n",
    "# df_merged = df_nhanes.merge(lmf_clean, on=\"SEQN\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f06fe3-d4a8-445a-96b3-92ec4bd83374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged.head()\n",
    "#df_merged.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3276f33c-22d3-4019-98e0-583a81748cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up KM plots\n",
    "\n",
    "# quick sanity checks to start\n",
    "#df_merged[[\"death\",\"months_followup\"]].describe(include=\"all\")\n",
    "\n",
    "\n",
    "#df_merged[[\"mortstat\",\"permth_exm\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b245789-43a8-4f64-a23f-f69fba37f398",
   "metadata": {},
   "source": [
    "## resume coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3b0ddf-0407-4403-a23e-9e4d9b40e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (9971, 471)\n",
      "After dropping survey weights: (9971, 465)\n",
      "Numeric matrix shape: (9971, 463)\n",
      "Dropping 98 nearly-empty variables.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m     imputer = KNNImputer(n_neighbors=\u001b[32m7\u001b[39m, weights=\u001b[33m\"\u001b[39m\u001b[33mdistance\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     X_imputed_arr = \u001b[43mimputer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_impute_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     X_imputed = pd.DataFrame(X_imputed_arr,\n\u001b[32m     77\u001b[39m                              index=X_impute_only.index,\n\u001b[32m     78\u001b[39m                              columns=X_impute_only.columns)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mKNN Imputation successful.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/envs/py314/lib/python3.14/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/envs/py314/lib/python3.14/site-packages/sklearn/base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/envs/py314/lib/python3.14/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/envs/py314/lib/python3.14/site-packages/sklearn/impute/_knn.py:376\u001b[39m, in \u001b[36mKNNImputer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[32m    368\u001b[39m gen = pairwise_distances_chunked(\n\u001b[32m    369\u001b[39m     X[row_missing_idx, :],\n\u001b[32m    370\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_X,\n\u001b[32m   (...)\u001b[39m\u001b[32m    374\u001b[39m     reduce_func=process_chunk,\n\u001b[32m    375\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# process_chunk modifies X in place. No return value.\u001b[39;49;00m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keep_empty_features:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/envs/py314/lib/python3.14/site-packages/sklearn/metrics/pairwise.py:2249\u001b[39m, in \u001b[36mpairwise_distances_chunked\u001b[39m\u001b[34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[39m\n\u001b[32m   2247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2248\u001b[39m     chunk_size = D_chunk.shape[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2249\u001b[39m     D_chunk = \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/envs/py314/lib/python3.14/site-packages/sklearn/impute/_knn.py:339\u001b[39m, in \u001b[36mKNNImputer.transform.<locals>.process_chunk\u001b[39m\u001b[34m(dist_chunk, start)\u001b[39m\n\u001b[32m    334\u001b[39m dist_subset = dist_chunk[dist_idx_map[receivers_idx] - start][\n\u001b[32m    335\u001b[39m     :, potential_donors_idx\n\u001b[32m    336\u001b[39m ]\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# receivers with all nan distances impute with mean\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m all_nan_dist_mask = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_subset\u001b[49m\u001b[43m)\u001b[49m.all(axis=\u001b[32m1\u001b[39m)\n\u001b[32m    340\u001b[39m all_nan_receivers_idx = receivers_idx[all_nan_dist_mask]\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_nan_receivers_idx.size:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# generate ML-ready matrix\n",
    "# clean, imputed, standardized --> production-grade pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# 0. Copy dataset\n",
    "# ============================================================\n",
    "\n",
    "df_ml = df_final.copy()        # <-- replace with your merged df name\n",
    "df_ml = df_ml.drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "print(\"Initial shape:\", df_ml.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 1. Identify variable types\n",
    "# ============================================================\n",
    "\n",
    "id_cols = [\"SEQN\"]\n",
    "\n",
    "# treat categorical vars as numeric but later one-hot if needed\n",
    "categorical_like = [\n",
    "    \"RIAGENDR\", \"RIDRETH3\", \"DMDMARTL\", \"DMDEDUC2\", \"DMDEDUC3\",\n",
    "    \"poverty_income_ratio\"\n",
    "]\n",
    "\n",
    "# Remove obvious non-ML columns (weights, design variables)\n",
    "drop_cols = [c for c in df_ml.columns if c.startswith((\"WT\", \"SDMV\"))]\n",
    "\n",
    "df_ml = df_ml.drop(columns=drop_cols)\n",
    "print(\"After dropping survey weights:\", df_ml.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Keep only numeric columns for ML\n",
    "# ============================================================\n",
    "\n",
    "numeric_cols = df_ml.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c not in id_cols]\n",
    "\n",
    "X = df_ml[id_cols + numeric_cols].copy()\n",
    "print(\"Numeric matrix shape:\", X.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Clean impossible/dummy values\n",
    "# ============================================================\n",
    "\n",
    "# Standard NHANES missing codes\n",
    "missing_codes = {\n",
    "    7777: np.nan, 777: np.nan, \n",
    "    9999: np.nan, 999: np.nan,\n",
    "    99999: np.nan, 999999: np.nan\n",
    "}\n",
    "\n",
    "X = X.replace(missing_codes)\n",
    "\n",
    "# Remove columns that are >95% missing (no value)\n",
    "missing_fraction = X.isna().mean()\n",
    "cols_to_drop = missing_fraction[missing_fraction > 0.95].index.tolist()\n",
    "\n",
    "print(f\"Dropping {len(cols_to_drop)} nearly-empty variables.\")\n",
    "X.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Imputation — KNN if possible, else median\n",
    "# ============================================================\n",
    "\n",
    "X_impute_only = X.set_index(\"SEQN\").copy()\n",
    "\n",
    "try:\n",
    "    imputer = KNNImputer(n_neighbors=7, weights=\"distance\")\n",
    "    X_imputed_arr = imputer.fit_transform(X_impute_only)\n",
    "\n",
    "    X_imputed = pd.DataFrame(X_imputed_arr,\n",
    "                             index=X_impute_only.index,\n",
    "                             columns=X_impute_only.columns)\n",
    "    print(\"KNN Imputation successful.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"KNN failed → Falling back to median imputation:\", e)\n",
    "    X_imputed = X_impute_only.fillna(X_impute_only.median())\n",
    "\n",
    "# Add back SEQN\n",
    "X_imputed = X_imputed.reset_index()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SAVE IMPUTED MATRIX HERE\n",
    "# ------------------------------------------------------------\n",
    "X_imputed.to_feather(\"X_imputed.feather\")\n",
    "print(\"Saved imputed dataset → X_imputed.feather\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Continue pipeline below this\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41dd66d-69a9-4f14-98a4-88d289a6cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4b. REDEFINE numeric columns AFTER cleaning & imputation\n",
    "# ============================================================\n",
    "\n",
    "# Recompute numeric columns from imputed dataset\n",
    "numeric_cols = [\n",
    "    c for c in X_imputed.columns\n",
    "    if c not in [\"SEQN\"]\n",
    "    and pd.api.types.is_numeric_dtype(X_imputed[c])\n",
    "]\n",
    "\n",
    "print(\"Final numeric column count:\", len(numeric_cols))\n",
    "\n",
    "# ============================================================\n",
    "# 5. Standardization (z-score)\n",
    "# ============================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_std_arr = scaler.fit_transform(X_imputed[numeric_cols])\n",
    "\n",
    "X_standardized = pd.DataFrame(\n",
    "    X_std_arr,\n",
    "    columns=numeric_cols,\n",
    "    index=X_imputed.index\n",
    ")\n",
    "\n",
    "# Add back SEQN\n",
    "X_standardized.insert(0, \"SEQN\", X_imputed[\"SEQN\"])\n",
    "\n",
    "print(\"\\n=== STANDARDIZATION COMPLETE ===\")\n",
    "print(\"X_standardized shape:\", X_standardized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac2373-99cd-471c-a729-79eca68226e5",
   "metadata": {},
   "source": [
    "note: all this returns 3 outputs:\n",
    "\n",
    "X_clean — cleaned but not imputed\n",
    "\n",
    "X_imputed — all numeric holes filled\n",
    "\n",
    "X_standardized — fully ML-ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20549a5e-3c74-472e-bcef-59cf454004a0",
   "metadata": {},
   "source": [
    "## build frailty index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee542e-a827-45fc-addf-22d80165396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# df_ml must contain the engineered variables:\n",
    "# age, BMI, fasting_glucose, hdl_cholesterol, triglycerides, ldl_cholesterol,\n",
    "# egfr, sleep_score, mvpa_min_week, phq9_total, etc.\n",
    "\n",
    "df_val = df_ml.copy()\n",
    "\n",
    "###########################################\n",
    "# Helper function for safe deficit creation\n",
    "###########################################\n",
    "def deficit_if(col, condition):\n",
    "    \"\"\"Return deficit (0/1) if col exists, else NaN.\"\"\"\n",
    "    if col not in df_val.columns:\n",
    "        return pd.Series(np.nan, index=df_val.index)\n",
    "    return condition(df_val[col]).astype(float)\n",
    "\n",
    "###########################################\n",
    "# Frailty deficit definitions\n",
    "###########################################\n",
    "frailty = pd.DataFrame(index=df_val.index)\n",
    "\n",
    "# ---- Metabolic & Lipid Deficits ----\n",
    "frailty[\"diabetes\"]       = deficit_if(\"fasting_glucose\", lambda x: x >= 126)\n",
    "frailty[\"low_hdl\"]        = deficit_if(\"hdl_cholesterol\", lambda x: x < 40)\n",
    "frailty[\"high_trig\"]      = deficit_if(\"triglycerides\", lambda x: x >= 150)\n",
    "frailty[\"high_ldl\"]       = deficit_if(\"ldl_cholesterol\", lambda x: x >= 160)\n",
    "\n",
    "# ---- Kidney Disease ----\n",
    "frailty[\"kidney_disease\"] = deficit_if(\"egfr\", lambda x: x < 60)\n",
    "\n",
    "# ---- BMI Deficits ----\n",
    "frailty[\"obesity\"]        = deficit_if(\"BMI\", lambda x: x >= 30)\n",
    "frailty[\"underweight\"]    = deficit_if(\"BMI\", lambda x: x < 18.5)\n",
    "\n",
    "# ---- Mental Health ----\n",
    "frailty[\"phq9_moderate\"]  = deficit_if(\"phq9_total\", lambda x: x >= 10)\n",
    "\n",
    "# ---- Activity & Function ----\n",
    "frailty[\"low_mvpa\"]       = deficit_if(\"mvpa_min_week\", lambda x: x < 150)\n",
    "\n",
    "# ---- Sleep ----\n",
    "frailty[\"poor_sleep\"]     = deficit_if(\"sleep_score\", lambda x: x < 80)\n",
    "\n",
    "# ---- Inflammation (CRP) ----\n",
    "if \"LBXCRP\" in df_val.columns:\n",
    "    df_val[\"crp\"] = df_val[\"LBXCRP\"]\n",
    "elif \"LBDCRP\" in df_val.columns:\n",
    "    df_val[\"crp\"] = df_val[\"LBDCRP\"]\n",
    "else:\n",
    "    df_val[\"crp\"] = np.nan\n",
    "\n",
    "frailty[\"crp_deficit\"]    = (df_val[\"crp\"] > 3).astype(float)\n",
    "\n",
    "###########################################\n",
    "# Compute Frailty Index (FI)\n",
    "###########################################\n",
    "frailty[\"deficits_present\"] = frailty.sum(axis=1)\n",
    "frailty[\"deficits_possible\"] = frailty.notna().sum(axis=1)\n",
    "frailty[\"frailty_index\"] = frailty[\"deficits_present\"] / frailty[\"deficits_possible\"]\n",
    "\n",
    "# Merge FI back into dataset\n",
    "df_val[\"frailty_index\"] = frailty[\"frailty_index\"]\n",
    "\n",
    "print(\"Frailty Index completed.\")\n",
    "df_val[[\"frailty_index\"]].head()\n",
    "\n",
    "\n",
    "# Keep only columns that exist and have > 30 non-missing values and >1 unique value\n",
    "reverse_vars = [\n",
    "    v for v in reverse_vars \n",
    "    if v in df_val.columns \n",
    "    and df_val[v].notna().sum() > 30 \n",
    "    and df_val[v].nunique() > 1\n",
    "]\n",
    "\n",
    "normal_vars = [\n",
    "    v for v in normal_vars\n",
    "    if v in df_val.columns\n",
    "    and df_val[v].notna().sum() > 30\n",
    "    and df_val[v].nunique() > 1\n",
    "]\n",
    "\n",
    "print(\"reverse_vars used:\", reverse_vars)\n",
    "print(\"normal_vars used:\", normal_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab10f3-61d3-4090-8135-b80dbce6dceb",
   "metadata": {},
   "source": [
    "## create longevity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165adaa8-7d34-4859-ac8b-d529a3ff7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_ml.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a37990-d8b5-44c4-90fe-11f7fb652d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower = better (reverse-coded)\n",
    "reverse_vars = [\n",
    "    \"fasting_glucose\",\n",
    "    \"homa_ir\",\n",
    "    \"ldl_cholesterol\",\n",
    "    \"triglycerides\",\n",
    "    \"apob_est\",\n",
    "    \"phq9_total\",\n",
    "    # NOTE: egfr is *not* here anymore\n",
    "]\n",
    "\n",
    "# Higher = better\n",
    "normal_vars = [\n",
    "    \"hdl_cholesterol\",\n",
    "    \"sleep_score\",\n",
    "    \"rpdqs_normalized\",\n",
    "    \"egfr\",       # moved here: high eGFR is good\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4563097-c139-48ab-b92e-be1db35ab242",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vars = [v for v in reverse_vars if v in df_val.columns]\n",
    "normal_vars  = [v for v in normal_vars  if v in df_val.columns]\n",
    "\n",
    "print(\"Reverse-coded:\", reverse_vars)\n",
    "print(\"Normal:\", normal_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06086b-9380-423c-9469-a08848110082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make sure df_val is not modified elsewhere\n",
    "df_val = df_val.copy()\n",
    "\n",
    "# ---- HIGHER = BETTER ----\n",
    "scaler_normal = StandardScaler()\n",
    "df_val[\"ls_normal\"] = (\n",
    "    pd.DataFrame(\n",
    "        scaler_normal.fit_transform(df_val[normal_vars]),\n",
    "        columns=normal_vars,\n",
    "        index=df_val.index\n",
    "    ).mean(axis=1)\n",
    ")\n",
    "\n",
    "# ---- LOWER = BETTER (reverse-coded) ----\n",
    "scaler_reverse = StandardScaler()\n",
    "df_val[\"ls_reverse\"] = (\n",
    "    pd.DataFrame(\n",
    "        scaler_reverse.fit_transform(df_val[reverse_vars]),\n",
    "        columns=reverse_vars,\n",
    "        index=df_val.index\n",
    "    ).mean(axis=1) * -1    # Reverse direction\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59007c3-2a96-449c-8153-06e816683f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[\"normalized_score\"] = df_val[[\"ls_normal\", \"ls_reverse\"]].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcffc2-0a70-4e28-bd15-4dceaa41e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_s = df_val[\"normalized_score\"].min()\n",
    "max_s = df_val[\"normalized_score\"].max()\n",
    "\n",
    "df_val[\"longevity_score\"] = 300 + (df_val[\"normalized_score\"] - min_s) * (\n",
    "    550 / (max_s - min_s)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e077972-6af9-4034-acd4-ec51d944b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"longevity_score\" in df_val.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd58ca-4e78-4cd6-8f90-6c6154e61d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[[\"ls_normal\",\"ls_reverse\",\"normalized_score\",\"longevity_score\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac55bf-2af2-4fe9-a81b-0c7b9914f983",
   "metadata": {},
   "source": [
    "## create frailty score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faad10-39f0-48ce-aa42-b0dfeaae4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Define deficit rules\n",
    "# ----------------------------\n",
    "\n",
    "frailty_defs = {}\n",
    "\n",
    "# 1. Cardiometabolic\n",
    "if \"bmi\" in df_val.columns:\n",
    "    frailty_defs[\"obesity\"] = (df_val[\"bmi\"] >= 30).astype(int)\n",
    "    frailty_defs[\"underweight\"] = (df_val[\"bmi\"] < 18.5).astype(int)\n",
    "\n",
    "if \"hdl_cholesterol\" in df_val.columns:\n",
    "    frailty_defs[\"low_hdl\"] = (df_val[\"hdl_cholesterol\"] < 40).astype(int)\n",
    "\n",
    "if \"ldl_cholesterol\" in df_val.columns:\n",
    "    frailty_defs[\"high_ldl\"] = (df_val[\"ldl_cholesterol\"] > 160).astype(int)\n",
    "\n",
    "# 2. Inflammation\n",
    "if \"crp\" in df_val.columns:\n",
    "    frailty_defs[\"high_crp\"] = (df_val[\"crp\"] > 3).astype(int)\n",
    "\n",
    "# 3. Metabolic\n",
    "if \"fasting_glucose\" in df_val.columns:\n",
    "    frailty_defs[\"prediabetes\"] = (df_val[\"fasting_glucose\"] >= 100).astype(int)\n",
    "\n",
    "if \"homa_ir\" in df_val.columns:\n",
    "    frailty_defs[\"insulin_resistance\"] = (df_val[\"homa_ir\"] > 2.0).astype(int)\n",
    "\n",
    "# 4. Kidney function\n",
    "if \"egfr\" in df_val.columns:\n",
    "    frailty_defs[\"ckd\"] = (df_val[\"egfr\"] < 60).astype(int)\n",
    "\n",
    "# 5. Mental health\n",
    "if \"phq9_total\" in df_val.columns:\n",
    "    frailty_defs[\"depression\"] = (df_val[\"phq9_total\"] >= 10).astype(int)\n",
    "\n",
    "# 6. Physical activity\n",
    "if \"mvpa_min_week\" in df_val.columns:\n",
    "    frailty_defs[\"low_activity\"] = (df_val[\"mvpa_min_week\"] < 150).astype(int)\n",
    "\n",
    "# Convert rules into DataFrame\n",
    "frailty_df = pd.DataFrame(frailty_defs)\n",
    "\n",
    "# Frailty index = mean of deficits (row-wise)\n",
    "df_val[\"frailty_index\"] = frailty_df.mean(axis=1)\n",
    "\n",
    "print(\"Frailty index created — preview:\")\n",
    "df_val[[\"SEQN\",\"frailty_index\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111b712-bb35-4348-b8a4-748883bf9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"frailty_index\" in df_val.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ccab6-520e-405b-bb51-000f69376d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[\"frailty_index\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60229d6f-31d1-421b-b137-5997e6ffc77c",
   "metadata": {},
   "source": [
    "## compare longevity score and frailty index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9312e70-5a99-4786-a512-b6199adfbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = df_val[[\"longevity_score\", \"frailty_index\", \"age\"]].dropna()\n",
    "\n",
    "corr = val[\"longevity_score\"].corr(val[\"frailty_index\"])\n",
    "print(\"Correlation:\", corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89943685-d368-4e4b-a7e4-93a8f1210ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(\n",
    "    data=val,\n",
    "    x=\"longevity_score\",\n",
    "    y=\"frailty_index\",\n",
    "    scatter_kws={\"alpha\":0.3, \"s\":20}\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c111a-f18b-443a-90ea-44fff4b942b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val[\"LS_quartile\"] = pd.qcut(val[\"longevity_score\"], 4, labels=[\"Q1 Low\",\"Q2\",\"Q3\",\"Q4 High\"])\n",
    "\n",
    "val.groupby(\"LS_quartile\")[\"frailty_index\"].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eccda0d-0e96-4373-8359-7c5a4b4a400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(\n",
    "    data=val,\n",
    "    x=\"LS_quartile\",\n",
    "    y=\"frailty_index\",\n",
    "    order=[\"Q1 Low\",\"Q2\",\"Q3\",\"Q4 High\"]\n",
    ")\n",
    "plt.title(\"Frailty Index by Longevity Score Quartile\")\n",
    "plt.ylabel(\"Frailty Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f226f33-eae5-427c-919a-d1d64b78ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = df_val[[\"SEQN\", \"longevity_score\", \"frailty_index\"]].dropna()\n",
    "val[\"LS_quartile\"] = pd.qcut(val[\"longevity_score\"], 4, labels=[\"Q1 Low\",\"Q2\",\"Q3\",\"Q4 High\"])\n",
    "\n",
    "# top frail cases in Q4 (best LS)\n",
    "q4_bad = (\n",
    "    val[val[\"LS_quartile\"] == \"Q4 High\"]\n",
    "    .sort_values(\"frailty_index\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "q4_bad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1deb68-64c0-4b41-9956-d27d7e3e0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild val\n",
    "val = df_val[[\"SEQN\", \"longevity_score\", \"frailty_index\", \"age\"]].dropna()\n",
    "val[\"LS_quartile\"] = pd.qcut(val[\"longevity_score\"], 4,\n",
    "                             labels=[\"Q1 Low\", \"Q2\", \"Q3\", \"Q4 High\"])\n",
    "\n",
    "print(val.groupby(\"LS_quartile\")[\"frailty_index\"].mean())\n",
    "\n",
    "# Re-check the \"worst\" Q4 cases\n",
    "q4_bad = (\n",
    "    val[val[\"LS_quartile\"] == \"Q4 High\"]\n",
    "    .sort_values(\"frailty_index\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "q4_bad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f118609-5dc2-4c04-ae78-c86d63d94a58",
   "metadata": {},
   "source": [
    "# saving / reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818ecb4-a3aa-4d80-afbc-fb2fded2685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "# df_val.to_pickle(\"df_val.pkl\")\n",
    "\n",
    "# to load\n",
    "import pandas as pd\n",
    "\n",
    "df_val = pd.read_pickle(\"df_val.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace26d41-a0a9-447f-b522-27d7ea4fabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec94f5f-7d85-4ee3-9ede-6c4a16a1118b",
   "metadata": {},
   "source": [
    "## validation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c1a1c-cc3a-4c97-a937-d9db4dc7c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optional: LOWESS if statsmodels is installed\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    HAS_SM = True\n",
    "except ImportError:\n",
    "    HAS_SM = False\n",
    "    print(\"statsmodels not found – LOWESS curve will be skipped.\")\n",
    "\n",
    "\n",
    "def run_longevity_frailty_validation(df_val, frailty_cut=0.25):\n",
    "    \"\"\"\n",
    "    Validation pipeline:\n",
    "    - Cleans data\n",
    "    - Builds frailty groups\n",
    "    - Computes correlation, Cohen's d, eta^2\n",
    "    - Makes fast, publication-ready plots\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1. Minimal working dataset\n",
    "    # --------------------------------------------------------\n",
    "    required = [\"SEQN\", \"longevity_score\", \"frailty_index\", \"age\"]\n",
    "    missing = [c for c in required if c not in df_val.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns missing: {missing}\")\n",
    "\n",
    "    val = df_val[required].dropna().copy()\n",
    "    print(f\"Validation N after dropna: {len(val):,}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2. Frailty groups and LS quartiles\n",
    "    # --------------------------------------------------------\n",
    "    val[\"frail_group\"] = np.where(\n",
    "        val[\"frailty_index\"] >= frailty_cut,\n",
    "        \"Frail (FI ≥ {:.2f})\".format(frailty_cut),\n",
    "        \"Non-frail\"\n",
    "    )\n",
    "\n",
    "    # LS quartiles\n",
    "    val[\"LS_quartile\"] = pd.qcut(\n",
    "        val[\"longevity_score\"],\n",
    "        4,\n",
    "        labels=[\"Q1 (lowest)\", \"Q2\", \"Q3\", \"Q4 (highest)\"]\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3. Descriptive summaries\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n--- Descriptives ---\")\n",
    "    print(val[[\"longevity_score\", \"frailty_index\", \"age\"]].describe())\n",
    "\n",
    "    print(\"\\nFrailty group sizes:\")\n",
    "    print(val[\"frail_group\"].value_counts(normalize=False))\n",
    "    print(\"\\nFrailty group proportions:\")\n",
    "    print(val[\"frail_group\"].value_counts(normalize=True).round(3))\n",
    "\n",
    "    print(\"\\nFrailty by Longevity Score quartile (mean FI):\")\n",
    "    print(val.groupby(\"LS_quartile\")[\"frailty_index\"].mean().round(3))\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4. Correlation: Longevity Score vs Frailty Index\n",
    "    # --------------------------------------------------------\n",
    "    r = val[\"longevity_score\"].corr(val[\"frailty_index\"])\n",
    "    print(\"\\nPearson correlation (LS vs FI): {:.3f}\".format(r))\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5. Effect size: Cohen's d (frail vs non-frail)\n",
    "    # --------------------------------------------------------\n",
    "    frail = val.loc[val[\"frail_group\"].str.startswith(\"Frail\"), \"longevity_score\"]\n",
    "    non_frail = val.loc[val[\"frail_group\"] == \"Non-frail\", \"longevity_score\"]\n",
    "\n",
    "    m_frail = frail.mean()\n",
    "    m_non  = non_frail.mean()\n",
    "    s_frail = frail.std()\n",
    "    s_non   = non_frail.std()\n",
    "\n",
    "    # pooled SD\n",
    "    n_frail = len(frail)\n",
    "    n_non   = len(non_frail)\n",
    "    s_pooled = np.sqrt(\n",
    "        ((n_frail - 1) * s_frail**2 + (n_non - 1) * s_non**2) /\n",
    "        (n_frail + n_non - 2)\n",
    "    )\n",
    "    d = (m_non - m_frail) / s_pooled\n",
    "\n",
    "    print(\"\\n--- Cohen's d (Non-frail vs Frail) ---\")\n",
    "    print(f\"Mean LS (Non-frail): {m_non:.2f}\")\n",
    "    print(f\"Mean LS (Frail):     {m_frail:.2f}\")\n",
    "    print(f\"Cohen's d:           {d:.3f}  (positive = higher LS in non-frail)\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6. Effect size: η² for LS by frailty group (one-way ANOVA)\n",
    "    # --------------------------------------------------------\n",
    "    y = val[\"longevity_score\"].values\n",
    "    grand_mean = y.mean()\n",
    "\n",
    "    # Group-wise\n",
    "    group_means = val.groupby(\"frail_group\")[\"longevity_score\"].mean()\n",
    "    group_sizes = val[\"frail_group\"].value_counts()\n",
    "\n",
    "    # Between-group SS\n",
    "    ss_between = 0.0\n",
    "    for g in group_means.index:\n",
    "        ss_between += group_sizes[g] * (group_means[g] - grand_mean) ** 2\n",
    "\n",
    "    # Total SS\n",
    "    ss_total = ((y - grand_mean) ** 2).sum()\n",
    "\n",
    "    eta_sq = ss_between / ss_total\n",
    "\n",
    "    print(\"\\n--- ANOVA effect size (η²) ---\")\n",
    "    print(\"η² (frail_group → LS): {:.3f}\".format(eta_sq))\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 7. Boxplot + sampled jitter (fast alternative to swarm)\n",
    "    # --------------------------------------------------------\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.boxplot(\n",
    "        data=val,\n",
    "        x=\"frail_group\",\n",
    "        y=\"longevity_score\"\n",
    "    )\n",
    "\n",
    "    # jitter a sample of points\n",
    "    jitter_sample = val.sample(min(800, len(val)), random_state=42)\n",
    "    sns.stripplot(\n",
    "        data=jitter_sample,\n",
    "        x=\"frail_group\",\n",
    "        y=\"longevity_score\",\n",
    "        color=\"black\",\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    plt.title(\"Longevity Score by Frailty Group\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"Longevity Score (300–850 scale)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(\"fig_ls_frail_groups.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    # --------------------------------------------------------\n",
    "    # 8. Boxenplot across LS quartiles (FI distribution)\n",
    "    # --------------------------------------------------------\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.boxenplot(\n",
    "        data=val,\n",
    "        x=\"LS_quartile\",\n",
    "        y=\"frailty_index\"\n",
    "    )\n",
    "    plt.title(\"Frailty Index by Longevity Score Quartile\")\n",
    "    plt.xlabel(\"Longevity Score Quartile\")\n",
    "    plt.ylabel(\"Frailty Index\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(\"fig_ls_fi_quartile.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    # --------------------------------------------------------\n",
    "    # 9. Scatter LS vs FI with optional LOWESS\n",
    "    # --------------------------------------------------------\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(\n",
    "        val[\"frailty_index\"],\n",
    "        val[\"longevity_score\"],\n",
    "        alpha=0.25,\n",
    "        s=10\n",
    "    )\n",
    "    plt.xlabel(\"Frailty Index\")\n",
    "    plt.ylabel(\"Longevity Score (300–850)\")\n",
    "    plt.title(\"Longevity Score vs Frailty Index\")\n",
    "\n",
    "    if HAS_SM:\n",
    "        # LOWESS smoothing\n",
    "        lowess = sm.nonparametric.lowess\n",
    "        smoothed = lowess(\n",
    "            val[\"longevity_score\"],\n",
    "            val[\"frailty_index\"],\n",
    "            frac=0.3\n",
    "        )\n",
    "        plt.plot(smoothed[:, 0], smoothed[:, 1], linewidth=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(\"fig_ls_fi_scatter.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    # --------------------------------------------------------\n",
    "    # 10. Return the working frame for further analysis\n",
    "    # --------------------------------------------------------\n",
    "    return val\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN THE PIPELINE\n",
    "# ============================================================\n",
    "val_results = run_longevity_frailty_validation(df_val, frailty_cut=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07539bbd-06aa-4c09-b852-576d25e0ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess distribution of frailty index\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df_val[\"frailty_index\"].dropna(), \n",
    "             bins=30, \n",
    "             kde=True, \n",
    "             color=\"steelblue\")\n",
    "\n",
    "plt.title(\"Distribution of Frailty Index (FI)\")\n",
    "plt.xlabel(\"Frailty Index\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"fig_fi_hist.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6023ea-0199-44a2-aac5-5ce131a731e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d1f8f-9519-4a1f-a4f7-b09fe927cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess distribution of longevity score \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df_val[\"longevity_score\"].dropna(),\n",
    "             bins=30,\n",
    "             kde=True,\n",
    "             color=\"darkgreen\")\n",
    "\n",
    "plt.title(\"Distribution of Longevity Score\")\n",
    "plt.xlabel(\"Longevity Score (300–850 scale)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"fig_ls_hist.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
